{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Ideal bandwidths and PDF functions \n",
    "\n",
    "Based on kde_tutorial from https://github.com/saint-germain/population_synthesis/blob/master/tools/kde_tutorial.ipynb\n",
    "\n",
    "## Libraries and data\n",
    "The data trataiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_png {}\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {}\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "plt.style.use('./images/img.mplstyle')\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}', r'\\usepackage{wasysym}']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn=pd.read_csv('data/no_p.csv',index_col=None) #without pertubtations \n",
    "dn[\"gia\"]=dn.ngi>0\n",
    "dl=pd.read_csv('data/low_p.csv',index_col=None) #with low pertubtations \n",
    "dl[\"gia\"]=dl.ngi>0\n",
    "dh=pd.read_csv('data/high_p.csv',index_col=None) #with high pertubtations \n",
    "dh[\"gia\"]=dh.ngi>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Terrestrial\n",
    "dnt=dn[~dn[\"gia\"]] # without pertubtations \n",
    "dlt=dl[~dl[\"gia\"]] # low pertubtations \n",
    "dht=dh[~dh[\"gia\"]] # high pertubtations \n",
    "\n",
    "#Giant \n",
    "dng=dn[dn[\"gia\"]]  # without pertubtations \n",
    "dlg=dl[dl[\"gia\"]]  # low pertubtations \n",
    "dhg=dh[dh[\"gia\"]]  # high without pertubtations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"***************************************\")\n",
    "#print(\"Total systems                  : \"+str(len(dn)))\n",
    "#print(\"***************************************\")\n",
    "#print(\"Total planets - no             : \"+str(dn.nplanets.sum().astype(int)))\n",
    "#print(\"Giant planets - no             : \"+str(dn.ngi.sum().astype(int)))\n",
    "#print(\"Systems with giant planets - no: \"+str(dn.gia.sum()))\n",
    "#print(\"***************************************\")\n",
    "\n",
    "#print(\"Total planets - lo             : \"+str(dl.nplanets.sum().astype(int)))\n",
    "#print(\"Giant planets - lo             : \"+str(dl.ngi.sum().astype(int)))\n",
    "#print(\"Systems with giant planets - lo: \"+str(dl.gia.sum()))\n",
    "#print(\"***************************************\")\n",
    "#print(\"Total planets - hi             : \"+str(dh.nplanets.sum().astype(int)))\n",
    "#print(\"Giant planets - hi             : \"+str(dh.ngi.sum().astype(int)))\n",
    "#print(\"Systems with giant planets - hi: \"+str(dh.gia.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variables = [dng,dlg,dhg,dnt,dlt,dht,dn,dl,dh]\n",
    "\n",
    "for i, var in enumerate(x_variables):\n",
    "    #var['logeff'] = np.log10(var.massefficiency)\n",
    "    var['logcom'] = np.log10(var.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ms</th>\n",
       "      <th>metal</th>\n",
       "      <th>md</th>\n",
       "      <th>taugas</th>\n",
       "      <th>com</th>\n",
       "      <th>Mtp</th>\n",
       "      <th>Mjup</th>\n",
       "      <th>Mrock</th>\n",
       "      <th>nplanets</th>\n",
       "      <th>ngi</th>\n",
       "      <th>npt</th>\n",
       "      <th>gia</th>\n",
       "      <th>logcom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.880114</td>\n",
       "      <td>0.328146</td>\n",
       "      <td>0.056</td>\n",
       "      <td>1.644711e+06</td>\n",
       "      <td>1.081461</td>\n",
       "      <td>0.032311</td>\n",
       "      <td>13.536323</td>\n",
       "      <td>6454.676716</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.034011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.075269</td>\n",
       "      <td>-0.150160</td>\n",
       "      <td>0.110</td>\n",
       "      <td>1.014449e+06</td>\n",
       "      <td>2.932894</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.861989</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.467296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.861595</td>\n",
       "      <td>0.021273</td>\n",
       "      <td>0.120</td>\n",
       "      <td>1.790533e+06</td>\n",
       "      <td>0.110789</td>\n",
       "      <td>0.007267</td>\n",
       "      <td>0.634259</td>\n",
       "      <td>2217.900297</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.955504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.076658</td>\n",
       "      <td>-0.282408</td>\n",
       "      <td>0.150</td>\n",
       "      <td>6.017040e+06</td>\n",
       "      <td>5.740174</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.156892</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.758925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.206445</td>\n",
       "      <td>-0.174039</td>\n",
       "      <td>0.170</td>\n",
       "      <td>3.956708e+06</td>\n",
       "      <td>0.172211</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>0.702237</td>\n",
       "      <td>2678.844363</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.763939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ms     metal     md        taugas       com       Mtp       Mjup  \\\n",
       "0  0.880114  0.328146  0.056  1.644711e+06  1.081461  0.032311  13.536323   \n",
       "1  1.075269 -0.150160  0.110  1.014449e+06  2.932894  0.000054   0.000000   \n",
       "2  0.861595  0.021273  0.120  1.790533e+06  0.110789  0.007267   0.634259   \n",
       "3  1.076658 -0.282408  0.150  6.017040e+06  5.740174  0.000024   0.000000   \n",
       "4  1.206445 -0.174039  0.170  3.956708e+06  0.172211  0.008716   0.702237   \n",
       "\n",
       "         Mrock  nplanets  ngi   npt    gia    logcom  \n",
       "0  6454.676716      10.0  4.0   6.0   True  0.034011  \n",
       "1    17.861989      12.0  0.0  12.0  False  0.467296  \n",
       "2  2217.900297      12.0  1.0  11.0   True -0.955504  \n",
       "3     8.156892       9.0  0.0   9.0  False  0.758925  \n",
       "4  2678.844363      13.0  1.0  12.0   True -0.763939  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bandwidth problem\n",
    "<p style='text-align: justify;'> \n",
    "So far we have our data. Now we need to construct their probability density distributions. To do this we employ Kernel Density Estimation (KDE). Let's study how to estimate the shape $\\widehat{f}_{h}(x)$ of the funtion which could represents the probability density function: \n",
    "\n",
    "$$\\widehat{f}_{h}(x)=\\frac{1}{n} \\sum_{i=1}^{n} K_{h}\\left(x-x_{i}\\right)=\\frac{1}{n h} \\sum_{i=1}^{n} K\\left(\\frac{x-x_{i}}{h}\\right).$$\n",
    "\n",
    "Here $K_{h}$ is a kernel (non-negative function), commonly used: uniform, triangular, biweight, triweight, Epanechnikov, normal, among others. The $h$ parameters is a smoothing constant, it is also called _bandwidth_ (also $bw$) (more information of KDE <a href=\"https://cran.r-project.org/web/packages/kedd/vignettes/kedd.pdf\">here</a>). \n",
    " \n",
    "\n",
    "### The Bandwidth definition\n",
    "A very brief definition, from <a href=\"https://en.wikipedia.org/wiki/Kernel_density_estimation\">wikipedia</a>: The bandwidth of a kernel is a free parameter which exhibits a strong influence on the resulting estimate. Basically, it determines the smoothness of the density function. The most common optimality criterion used to select this parameter is the expected risk function: \n",
    "\n",
    "$$\\operatorname{MISE}(h)=\\mathrm{E}\\left[\\int\\left(\\hat{f}_{h}(x)-f(x)\\right)^{2} d x\\right].$$\n",
    "\n",
    "Also is used the rule-of-thumb bandwidth estimator:\n",
    "\n",
    "$$h=\\left(\\frac{4 \\hat{\\sigma}^{5}}{3 n}\\right)^{\\frac{1}{5}} \\approx 1.06 \\hat{\\sigma} n^{-1 / 5}.$$\n",
    "\n",
    "Or Silverman's (1986) rule of thumb:\n",
    "\n",
    "$$h=0.9 \\min \\left(\\hat{\\sigma}, \\frac{I Q R}{1.34}\\right) n^{-\\frac{1}{5}}.$$\n",
    "\n",
    "Some considrations in the values of $h$. On the one hand, if $h\\rightarrow 0$ then we have overfitting, in other words there is no smoothing (plots like sum of delta fuctions).  On the other hand, if $h\\rightarrow \\infty$, we have a density function completily smoothed.\n",
    "\n",
    "Now, let's estudy the optimal $h$ ($bw$) using `GridsearshCV` with `cross_validation` for the `KernelDensity` method.  Also we consider `gaussian_kde` to compare the PDF results. We introduce a new funtoion that obtain the bandwidths through `Gaussian_kde` and `KernelDensity`.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The variable is a column from dataFrames: 'dng','dlg','dhg','dnt','dlt','dht','dn','dl','dh'\n",
    "def bw_opt(variable, name, plot=False):\n",
    "\n",
    "    m=.2; dvariable=0.05; deciamls=2\n",
    "    \n",
    "    x = variable/variable.min() \n",
    "    x_grid = np.around(np.arange(x.min()-m,x.max()+m,dvariable),deciamls)\n",
    "    norm, size, cv = 10, 30, 30 # 30-fold cross-validation     \n",
    "        \n",
    "    #-------------With CrossValidation:\n",
    "    #x = \n",
    "    grid = GridSearchCV(KernelDensity(), \n",
    "                        {'bandwidth': np.linspace(x.min(),x.max()/\\\n",
    "                                                  norm,size)},\n",
    "                        cv=cv) # max-min (grid)\n",
    "    grid.fit(x[:, None])\n",
    "    kde_CV = grid.best_estimator_\n",
    "    bw_CV  = list(grid.best_params_.values())[0]\n",
    "    pdf_CV = np.exp(kde_CV.score_samples(x_grid[:,None]))\n",
    "    \n",
    "    #-------------Gaussian_KDE:\n",
    "    kde_G = gaussian_kde(x)\n",
    "    f = kde_G.covariance_factor()\n",
    "    bw_KDE =  f * x.std()\n",
    "    pdf_kde = kde_G.evaluate(x_grid)\n",
    "    \n",
    "    #---------Comparative plot: \n",
    "    if plot == True: \n",
    "        fig, ax = plt.subplots(1,2, figsize=(11, 4), constrained_layout=True, sharex=True, sharey=True)\n",
    "        #-------CV_plot:\n",
    "        ax[0].plot(x_grid, pdf_CV, linewidth=2, label='bw=%.4f'%kde_CV.bandwidth)\n",
    "        ax[0].hist(x,25,density=True, alpha=.3)\n",
    "        ax[0].legend(loc=\"upper right\")\n",
    "        ax[0].set_title(\"Cross Validation\")\n",
    "        #-------kde_plot:\n",
    "        ax[1].plot(x_grid, pdf_kde, linewidth=2, label='bw=%.4f'%bw_KDE)\n",
    "        ax[1].hist(x,25,density=True, alpha=.3)\n",
    "        ax[1].legend(loc=\"upper right\")\n",
    "        ax[1].set_title(\"Gausian\\_kde\")\n",
    "        #-------sns_plot:\n",
    "        #sns.distplot(x,  ax=ax[2], axlabel=False, bins=8,\n",
    "        #             hist_kws={\"alpha\": .3, \"color\": 'C1'}, \n",
    "        #             label='default distplot') \n",
    "        #ax[2].legend(loc=\"upper right\")\n",
    "        #ax[2].set_title(\"displot\")\n",
    "        #---------------\n",
    "        fig.suptitle(\"variable: \"+name, \n",
    "                     fontsize=15, horizontalalignment='center')\n",
    "        plt.show()\n",
    "    return [bw_CV, bw_KDE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dng_logcom = bw_opt(dh.npt, name=\"dng.logcom\", plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the bw for all data, we can uncomment the following lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#x_variables = [dn,dl,dh]\n",
    "#names       = [\"dn\",\"dl\",\"dh\"]\n",
    "#opt_bw_eff  = [] \n",
    "#opt_bw_logcom  = [] \n",
    "\n",
    "#for index,variable in enumerate(x_variables):\n",
    "#    print(names[index]+\".\"+str(variable.com.name))\n",
    "#    bw = bw_opt(variable.com, names[index], plot=True)\n",
    "#    opt_bw_eff.append(bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, the results show overfiting, so, it is nessesary to study the bandwidth problem more thoroughly. We implement a seach vector in order to find the scalar hyperparameter of bandwidth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal bandwidth $bw$ and the PDF\n",
    "<p style='text-align: justify;'> \n",
    "    \n",
    "Now, we stablish a better way to determinie the $bw$ and the PDF for each variable. We implement the `KernelDansity` (through `SearchGridCV`) and `gaussian_kde` as methods of a new class `optimal_bw`. Here we will standarize the data, otherwise, variables measured at different scales do not contribute equally to the analysis. It also heps to determine the ideal bandwith. Therefore, \n",
    "\n",
    "- **Standarization**: It is another scaling technique where the values are centered around the mean ($\\mu=0$) with a unit as standard deviation ($\\sigma=1$). It is important when we compare measurements that have different units, because equalize the range and/or data variability, \n",
    "\n",
    "\\begin{equation}\n",
    "X^{\\prime}=\\frac{X-\\mu}{\\sigma}.\n",
    "\\end{equation}\n",
    "\n",
    "#### GridSearchCV Method\n",
    "We use `GridSearchCV` to estimate the otimal bandwidth $bw$. As the best splitting strategy in cross validation, we take cv$=10$, according with documentaction split the data in more grups is not orthodox, because it does not allow the method to learn with a larger data set.\n",
    "\n",
    "Finally, in the grid search we employ `KernelDensity` with a Gaussian kernel to generate the PDF for the variable with the ideal $bw$.  \n",
    "\n",
    "###### The search vector\n",
    "In the `GridSearchCV` method the optimal $bw$ comes form a scalar hyperparameter, so we introduce it as unidimentional vector. The minimum distance between data is the initial value in the serarch vector. As maximum value of serach we use $3\\sigma$ of our data (remember that data are standarized). Finally, the number of steps in the search vector is taken as the ideal for the score of `KernelDensity`.   \n",
    "\n",
    "#### Gaussian_kde  Method\n",
    "We use this method as is described in the documentation. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimal_bw(object):\n",
    "    \n",
    "    def __init__(self, variable):\n",
    "        \n",
    "        length, decimals = 100, 2 #some constants for the grid of space\n",
    "        self.variable = variable.values   #original variable\n",
    "        \n",
    "        self.o_grid = np.around(np.linspace(self.variable.min(), \n",
    "                                            self.variable.max(),\n",
    "                                            length), decimals)\n",
    "        \n",
    "        #standarization:\n",
    "        self.x = (self.variable-self.variable.mean())/self.variable.std()\n",
    "        #grid in the std space:\n",
    "        self.x_grid  = np.around(np.linspace(self.x.min(),\n",
    "                                             self.x.max(),\n",
    "                                             length), decimals)\n",
    "        \n",
    "        #distance between data\n",
    "        m = np.abs(np.diff(self.x))      \n",
    "        self.min = min(m[m>0])\n",
    "        \n",
    "    def cv_pdf(self):\n",
    "        \n",
    "        cv, nsteps = 10, 100               #some constants \n",
    "        vector = {'bandwidth': np.linspace(self.min, 3*self.x.std(), nsteps)}\n",
    "        \n",
    "        grid = GridSearchCV(KernelDensity(), vector, cv=cv)\n",
    "        grid.fit(self.x[:, None])\n",
    "        self.bw_CV  = grid.best_estimator_.bandwidth # bw in the standarized space       \n",
    "        self.pdf_CV = np.exp(grid.best_estimator_.score_samples(self.x_grid[:,None]))\n",
    "        \n",
    "        #KernelDensity in the variable space with the respective bw\n",
    "        self.bw_1 = self.bw_CV*self.variable.std() # bw in the variable space\n",
    "        kde_org = KernelDensity(bandwidth = self.bw_1).fit(self.variable[:, None])\n",
    "        self.PDF_1 = np.exp(kde_org.score_samples(self.o_grid[:, None]))\n",
    "    \n",
    "    def kde_pdf(self):\n",
    "        self.kde_G = gaussian_kde(self.x)\n",
    "        \n",
    "        self.bw_KDE =  self.kde_G.covariance_factor()      # bw in the standarized space\n",
    "        #self.bw_2 = self.bw_KDE*self.variable.std()   # bw in the variable space\n",
    "        self.pdf_kde = self.kde_G.evaluate(self.x_grid)\n",
    "        \n",
    "        kde_G2 = gaussian_kde(ex.variable)#*ex.variable.std(ddof = 1)) \n",
    "        self.pdf_kde2 = kde_G2.evaluate(ex.o_grid)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we implement the methods from `bw_optimal` as, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = optimal_bw(dn.metal)  #dng.logeff, dng.com, dng.metal\n",
    "ex.cv_pdf(); \n",
    "ex.kde_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figs(data, x, y1, y2, std = False):\n",
    "    \n",
    "    if std == True:\n",
    "        x_name, y_name = r\"Standarized metallicity $\\mu^{\\prime}$\", r\"$p(\\mu^{\\prime})$\"\n",
    "        color, a = \"C1\", .3\n",
    "        lims = [x.min(), x.max()]\n",
    "        name = \"comparative_std.pdf\"\n",
    "    else: \n",
    "        x_name, y_name = r\"Metallicity $\\mu$\", r\"$p(\\mu)$\"\n",
    "        color, a = \"C0\", .5\n",
    "        lims = [x.min(), x.max()]\n",
    "        name = \"comparative_real.pdf\"\n",
    "    \n",
    "    nbins = 25\n",
    "    fig, ax = plt.subplots(constrained_layout=True, sharey=True)\n",
    "    \n",
    "    ax.set_xlim(lims[0], lims[1])\n",
    "    ax.set_ylabel(y_name)\n",
    "    ax.set_xlabel(x_name)\n",
    "   \n",
    "    plt.hist(data, nbins, density=True, color = color, alpha = a, label = \"Synthetic data\")\n",
    "    \n",
    "    first_legend = plt.legend(loc = 0)\n",
    "    \n",
    "    ax = plt.gca().add_artist(first_legend)\n",
    "    kde1, = plt.plot(x, y2, ls = \"-\", lw = 3, color = \"C2\", alpha=.8, label = r\"$\\mathtt{gaussian\\_KDE}$\")\n",
    "    kde2, = plt.plot(x, y1, ls = \"--\", lw = 3, color = \"C4\", label = r\"$\\mathtt{KernelDensity}$\")\n",
    "    \n",
    "    plt.legend(handles=[kde1,kde2], bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "   \n",
    "    plt.savefig(\"images/plots/\"+name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figs(ex.variable, ex.o_grid, ex.PDF_1, ex.pdf_kde2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figs(ex.x, ex.x_grid, ex.pdf_CV, ex.pdf_kde, std = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we make a funtion that generates the comparative plot between `gaussian_kde` and `GridSearchCV` methods for get the pdf in the standarized data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the information in the variable `x_variables`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_variables = [dn,dl,dh]\n",
    "#for index,variable in enumerate(x_variables):\n",
    "#    #print(names[index]+\".\"+str(variable.logcom.name))\n",
    "#    bw = optimal_bw(variable.metal)\n",
    "#    bw.cv_pdf()\n",
    "#    bw.kde_pdf()\n",
    "#    plot = pdf1D_std(bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just plot the PDF obtained in the stanandarized space, in the real space we have some differences respect the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF in the variable space\n",
    "<p style='text-align: justify;'> \n",
    "In order to get the pdf in the variable space, we have two possibilities: \n",
    "\n",
    "1. Bring the PDF($X^\\prime$) to the original variable space, in the similar way that the reverse process as standarization for the variable with the PDF resulting in the methods of `optimal_bw` class: \n",
    "\n",
    "$$X=X^\\prime\\sigma+\\mu$$\n",
    "   \n",
    "   Then evaluate PDF($X$).Note that in this option we are not interested in bandwidth, just in take the PDF in the original space. \n",
    "\n",
    "2. Make the `KernelDensity` and `gaussian_kde` in the variable space, with the respective $bw$ obtained from standar space and transformed to the variable space. The bandwidth in the variable space $bw$ can be determined as,\n",
    "\n",
    "$$bw=\\sigma bw^\\prime.$$\n",
    "\n",
    "Here, the $bw^\\prime$ is the bandwidth in the standarized space, $\\sigma$ is the standar deviaton for our data. \n",
    "\n",
    "**Let's star with the first option:** \n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.around(np.linspace(ex.variable.min(),ex.variable.max(),100),2)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, constrained_layout=True, sharey=True)\n",
    "\n",
    "ax.plot(x,ex.pdf_CV, label=r'PDF($X$) - KernelDensity')\n",
    "ax.plot(x,ex.PDF_1, label=r'PDF($X$) - KernelDensity, $bw$ in real space', ls=\"--\")\n",
    "ax.plot(x,ex.pdf_kde, label=r'PDF($X$) - gaussian\\_kde')\n",
    "\n",
    "ax.hist(ex.variable, bins=25, color=\"C4\", density=True, alpha =.4, label=\"Real data\")\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "Therfore, the first option fails in the height of the PDF and in some cases also fails in the maximum values. It is a problem maybe because the traing data was in the standarized space, then when we change it to the original space the \"re-scaling\" through linear regression does not work.\n",
    "\n",
    "**Let's study the second option:**\n",
    "\n",
    "For the second option, we get the $bw^\\prime$ which is the bandwidth in the standarized space, to take it to the variable space, \n",
    "\n",
    "$$bw=\\sigma bw^\\prime.$$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid in the variable space\n",
    "x_space = np.around(np.linspace(ex.variable.min(),ex.variable.max(),100),2)\n",
    "\n",
    "#bw in the variable space\n",
    "BW_1 = ex.bw_CV*ex.variable.std()\n",
    "BW_2 = ex.bw_KDE/ex.variable.std(ddof=1)\n",
    "print(BW_1,BW_2)\n",
    "\n",
    "#pdf in the original space with the ideal bw employing Kernel Density\n",
    "kde_1d = KernelDensity(bandwidth = BW_1).fit(ex.variable[:, None])\n",
    "PDF_1  = np.exp(kde_1d.score_samples(x_space[:, None]))\n",
    "\n",
    "#pdf in the original space with the ideal bw employing Gaussian_KDE\n",
    "kde_G1d = gaussian_kde(ex.variable, bw_method = BW_2)\n",
    "PDF_2   = kde_G1d.evaluate(x_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(5.45, 3.9), constrained_layout=True, sharey=True)\n",
    "\n",
    "ax.hist(ex.variable, bins=25, color=\"C4\", density=True, alpha =.4, label=\"Real data\")\n",
    "plt.plot(x_space,PDF_1, label =\"KernelDensity,\" + \"bw=%.3f\"%BW_1)\n",
    "plt.plot(x_space,PDF_2, label =\"gaussian\\_kde,\" + \"bw=%.3f\"%BW_2)\n",
    "\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "Due to the $bw$ in the original space is the result of the product of the bandwidth in the standarized space $bw^{\\prime}$ times the standar deviation $\\sigma$, this problem could be dificult to solve for more than 1D. In other words, for 2D, 3D,..., $n$D, we have $n$ different deviations $\\sigma$, so what is the ideal to get the $bw$? We do not have idea of what is the optimal $\\sigma$ to get $bw$. Besides, the hyperparameter of $bw$ is a scaral no matter how many variables we have. \n",
    "\n",
    "However, we can score the best fit for `KernelDensity` using the differents values f $bw$ resulting of the product of the optimum in the standardized space $bw^\\prime$, times each of the data deviations $\\sigma_1,\\sigma_2,...,\\sigma_n$.\n",
    "</p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandwiths for  2-D and 3-D\n",
    "We are going to study the bw problem for more than one dimension. We will use `KernelDensity` to obtain the PDF for the variables in the standarized space PDF($X^\\prime$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar function that has standarized data and emply the methods in the class `optimal_bw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs two objects from class bw_optimal, which are normalized or standarized \n",
    "def bi_kde(data_x,data_y):\n",
    "    \n",
    "    x_grid = data_x.x_grid\n",
    "    y_grid = data_y.x_grid\n",
    "    m = [data_x.min,data_y.min]\n",
    "    \n",
    "    #grid = KernelDensity() # para hacer pruebas\n",
    "    \n",
    "    grid = GridSearchCV(KernelDensity(), \n",
    "                        {'bandwidth': np.linspace(min(m),\n",
    "                                                  3*data_x.x.std(),\n",
    "                                                  50)}, cv=10)\n",
    "    #xx = np.vstack([data_x.x,data_y.x]).T\n",
    "    #print(len(xx))\n",
    "    grid.fit(np.vstack([data_x.x,data_y.x]).T)\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    X, Y = np.meshgrid(x_grid, y_grid)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    #print(len(xy))\n",
    "    kde2d = grid.best_estimator_\n",
    "    pdf2d = np.exp(kde2d.score_samples(xy)).reshape(X.shape) #get the pdf\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.contourf(X, Y, pdf2d, levels=30, cmap=plt.cm.Reds)\n",
    "    \n",
    "    return pdf2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ex_2d = bi_kde(optimal_bw(dn.ms),optimal_bw(dn.metal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D example\n",
    "\n",
    "Just an example with 3 variables with standarized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------Data:\n",
    "x2 = (dn.ms-dn.ms.mean())/dn.ms.std()\n",
    "y2 = (dn.metal-dn.metal.mean())/dn.metal.std()\n",
    "z2 = (dn.nplanets-dn.nplanets.mean())/dn.nplanets.std()\n",
    "#z2 = dng.nplanets\n",
    "\n",
    "data2 = np.vstack([x2,y2,z2]).T\n",
    "\n",
    "#---------------------Grid:\n",
    "x2_grid=np.around(np.linspace(x2.min(),x2.max(),30),2)\n",
    "y2_grid=np.linspace(y2.min(),y2.max(),len(x2_grid))\n",
    "z2_grid=np.linspace(z2.min(),z2.max(),len(x2_grid))\n",
    "\n",
    "X2,Y2,Z2 = np.meshgrid(x2_grid, y2_grid, z2_grid)\n",
    "xyz2   = np.vstack([X2.ravel(), Y2.ravel(), Z2.ravel()]).T\n",
    "\n",
    "\n",
    "m=np.abs(np.diff([x2,y2,z2]))        #distance between data\n",
    "var_min=min(m[m>0])\n",
    "\n",
    "#----------------------CV:\n",
    "\n",
    "grid = GridSearchCV(KernelDensity(),\n",
    "                    {'bandwidth': np.linspace(var_min,3*x2.std(),50)},\n",
    "                    cv=20) # 20-fold cross-validation\n",
    "grid.fit(data2)\n",
    "\n",
    "kde3d_2 = grid.best_estimator_\n",
    "pdf3d_2 = np.exp(kde3d_2.score_samples(xyz2)).reshape(Z2.shape) #get the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of PDF($X^\\prime$), employing `plotly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.offline as offline\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "density_2 = pdf3d_2.ravel()\n",
    "\n",
    "fig = go.Figure(data=go.Volume(\n",
    "    x=X2.flatten(), y=Y2.flatten(), z=Z2.flatten(),\n",
    "    value=density_2.flatten(),\n",
    "    opacity=0.1,      # needs to be small to see through all surfaces\n",
    "    surface_count=30, # needs to be a large number for good volume rendering\n",
    "    colorscale = \"viridis\"\n",
    "    ))\n",
    "\n",
    "#fig.update_layout(scene_camera = dict(\n",
    "#    up=dict(x=0, y=0, z=1),\n",
    "#    center=dict(x=0, y=0, z=0),\n",
    "#    eye=dict(x=0.1, y=2.5, z=0.1)\n",
    "#))\n",
    "fig.update_layout(template=\"plotly\", width=800, height=800, \n",
    "                  scene_camera = dict(up=dict(x=0, y=0, z=1),\n",
    "                                      center=dict(x=0, y=0, z=0),\n",
    "                                      eye=dict(x=2, y=2, z=.1)),\n",
    "                 scene = dict(xaxis_title = \"Stellar mass $M_\\\\star$\",))\n",
    "                  \n",
    "fig.update_traces(colorbar_thickness=25, \n",
    "                  colorbar_len = .6, colorbar_x = .8, \n",
    "                  selector=dict(type='volume'))\n",
    "\n",
    "\n",
    "fig.write_image(\"images/plots/like1.pdf\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above examples for 2D and 3D were for the standarised data. Thus, it is nessesary bring them to the variable space according to the previous options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional case\n",
    "<p style='text-align: justify;'> \n",
    "It is time to obtain the bw and the PDF for any dimensionality. At this point, we use the `GridSearchCV` for the `KernelDensity` method, to obtain the ideal bw and the PDF function. \n",
    "\n",
    "The search vector has the same the same spirit: an initial value which is the minimum disntance betweeen the data and 3 times the standard deviation of our normalized data. Finally, the number of steps in the search vector is taken as the ideal for the optimal score of `KernelDensity`. \n",
    "\n",
    "Again, we will use as the best splitting strategy in cross validation cv$=10$.\n",
    "</p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimal_pdf(object):\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.lenght = 150\n",
    "        self.dim  = len(args)\n",
    "        self.data = np.vstack([*args]).T\n",
    "        self.data_std = (self.data-np.mean(self.data, axis=0))/np.std(self.data, axis=0) # standarization\n",
    "        \n",
    "    def grids(self): \n",
    "        #lenght is the size of space in 1D.  \n",
    "        self.real_interval, self.std_interval = [], []\n",
    "        \n",
    "        for i in range(self.dim):\n",
    "            self.real_interval.append(np.linspace(self.data[:,i].min(),\n",
    "                                                  self.data[:,i].max(),\n",
    "                                                  self.lenght))\n",
    "            self.std_interval.append(np.linspace(self.data_std[:,i].min(),\n",
    "                                                 self.data_std[:,i].max(),\n",
    "                                                 self.lenght))\n",
    "        \n",
    "        self.real_grid = np.meshgrid(*self.real_interval)        # PROBLEM MEMORY FOR 5D,6D... nD.\n",
    "        self.std_grid  = np.meshgrid(*self.std_interval)         # PROBLEM MEMORY FOR 5D,6D... nD.\n",
    "        # std\n",
    "        std = [self.std_grid[i].ravel() for i in range(len(self.std_grid))];    self.space_std = np.vstack([*std]).T\n",
    "        # real\n",
    "        real = [self.real_grid[i].ravel() for i in range(len(self.real_grid))]; self.space_real = np.vstack([*real]).T\n",
    "        \n",
    "    def bw(self):\n",
    "        # Variables:\n",
    "        lenght, cv = 50, 10  \n",
    "        dmin = np.abs(np.diff(self.data_std, \n",
    "                              axis=0))[np.abs(np.diff(self.data_std, \n",
    "                                                      axis=0))>0].min()\n",
    "        # Search vector\n",
    "        V_search = {'bandwidth':np.linspace(dmin, 3*self.data_std.std(), lenght)}\n",
    "        # GridSearchCV\n",
    "        self.grid_std = GridSearchCV(KernelDensity(), V_search, cv = cv).fit(self.data_std)\n",
    "        # bw\n",
    "        self.bw_std  = self.grid_std.best_estimator_.bandwidth\n",
    "    \n",
    "    def pdf(self):\n",
    "        # initialize the above methods \n",
    "        self.bw(); self.grids()\n",
    "        self.scores, kde_i = [], []\n",
    "        #std\n",
    "        self.pdf_std = np.exp(self.grid_std.best_estimator_.score_samples(self.space_std).reshape(self.std_grid[0].shape))        \n",
    "        #real (uncoment to generate the )\n",
    "        #bws  = self.bw_std*np.std(self.data, axis=0); \n",
    "        \n",
    "        #for i in range(len(bws)):\n",
    "        #    kde_i.append(KernelDensity(bandwidth=bws[i]).fit(self.data))\n",
    "        #    self.scores.append(kde_i[i].score(self.data))\n",
    "        # \n",
    "        #index = self.scores.index(max(self.scores))\n",
    "        #self.pdf_real = np.exp(kde_i[index].score_samples(self.space_real).reshape(self.real_grid[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "example = optimal_pdf(dn.com, dn.metal)#, dng.nplanets)#, dng.metal)\n",
    "#example.pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "example.grids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "example.pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we generate the plot for the 2D case, for the standar space: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(5,5), constrained_layout=True)\n",
    "\n",
    "lat = 15\n",
    "\n",
    "axs.set_title(r\"PDF in std space\", fontsize = lat)\n",
    "C1 = axs.contourf(example.std_grid[0], example.std_grid[1], example.pdf_std,\n",
    "                  levels=30, cmap=plt.cm.Reds)\n",
    "axs.set_xlabel(r'dng.logcom\\_std', fontsize = lat)\n",
    "axs.set_ylabel(r'dng.logeff\\_std', fontsize = lat)\n",
    "cbar = fig.colorbar(C1, ax = axs, aspect=25, alpha=1, ticks=[], pad=-.0)\n",
    "\n",
    "cbar.set_label(label = \"PDF with bw = %.3f\"%example.bw_std, fontsize=lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a comparative plot for the PDF in the real space and the standarized space:\n",
    "\n",
    "1. At left we just evaluate the PDF from the standarized space: PDF($X^\\prime$). \n",
    "2. At right, we plot the PDF obtained in the real space PDF($X$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(11,5), constrained_layout=True)\n",
    "\n",
    "lat = 15\n",
    "\n",
    "axs[0].set_title(\"PDF$(X^\\prime)$\", fontsize = lat)\n",
    "C1 = axs[0].contourf(example.std_grid[0],  example.std_grid[1], example.pdf_std, levels=30, cmap=plt.cm.Reds)\n",
    "axs[0].set_xlabel('dng.logcom\\_std', fontsize = lat)\n",
    "axs[0].set_ylabel('dng.logeff\\_std', fontsize = lat)\n",
    "fig.colorbar(C1, ax = axs[0], aspect=50, alpha=1,pad=-.0)\n",
    "\n",
    "axs[1].set_title(\"PDF$(X)$\", fontsize = lat)\n",
    "C2 = axs[1].contourf(example.real_grid[0], example.real_grid[1], example.pdf_std, levels=30, cmap=plt.cm.Reds)\n",
    "axs[1].set_xlabel('dng.logcom', fontsize = lat)\n",
    "fig.colorbar(C2, ax = axs[1], aspect=50, alpha=1,pad=-.0)\n",
    "\n",
    "#plt.contourf(XR, YR, PDF_p, levels=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior distribution and Marginalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's study  the prior distribution and the respective marginalization of an interest variable. To do this, we implement the notebook `Marginal_Disrtibutions.ipynb` in which we will import our class X `optimal_pdf` and the prior information about observed systems to get the posteriors distribution of physical characteristics of  plantetary systems. Thus, we will sum over the other variables to get the marginal distribution of planetary systems. \n",
    "\n",
    "\n",
    "The notebook `3D.ipynb` is just an example of the way to get the marginal dfistributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
